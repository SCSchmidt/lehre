---
title: "First tests in R"
author:
  - Sophie C. Schmidt:
      email: s.c.schmidt@uni-koeln.de
      correspondence: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  md_document:
    variant: markdown_github
always_allow_html: true
bibliography: references.bib
csl: RGK_archaeology_DGUF.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      include = TRUE)

```



# Tests!

## binomial test

This tests checks whether two nominal values, that are exclusive to each other, follow a theoretical distribution.

Let's imagine we've got a graveyard with 200 graves and we were able to determine exactly whether they were male or female. So there are 75 women and 125 men in our dataset.

These numbers make us think and we develop a hypothesis: There were as many women and men in the society that used the graveyard. We therefore believe that less women were entombed in this graveyard than men. But of course our excavation is only a sample, therefore we need to check whether that's statistically plausible:

From this we can develop the statistical hypothesis:

- H0 =  There is no difference between the graves of men and women in the population. The difference in our sample is by chance.
- H1 = non-directed: There are differences between the number of men and women interred in the graveyard. directed: Men were more often entombed than women

Der Binomialtest ist in R base umgesetzt und sehr leicht ausführbar. The general syntax is  `binom.test(nsuccesses, ntrials, p for successes, alternative="greater / less / two.sided")`. To translate this in our example:

nsuccesses would be men interred, ntrials are the number of graves we excavated, p is the statistical chance it should be a man (50%) and alternative (hypothesis) would be "greater" if we say number of successes is greater than expected, "less" if the number of successes are less than expected and "two.sided" if we do a non-directed test.

```{r binomial Bsp}
binom.test(125, 200, 0.5, alternative="greater")

# the other way round is just as possible:
binom.test(75, 200, 0.5, alternative = "less") # now the 75 Frauengräber are "successes" and therefore the alternative hypothesis needs to be turned around to "less"

# without a direction in the alternative hypothesis
binom.test(75, 200, 0.5, alternative = "two.sided")
```
Easy no?

Let's do this with a real archaeological data set. Let's just take the same question with the burial ground Ernest Witte in Texas:

```{r data sets}
data(EWBurials)
```
Maybe first have a look at what the data set consists of using `?EWburials`.

What we are going to do is write small functions inside our test-function. 
We first ask, how many burials are in the dataset alltogether (ntrials). For this we can use `nrow`:

```{r}
nrow(EWBurials)
```

Now we need to now, how many of those are of men? So we do a subset of `EWBurials`, where `EWBurials$Sex` equals "Male":

```{r}
nrow(EWBurials[EWBurials$Sex == "Male",])
```

don't forget the comma at the end: Remeber, using square brackets [,] enabled us to choose certain values from a table by selecting rows and columns. We can also add a condition. And we want to filter the rows for where the column Sex gives "Male", so we put this condition in the "rows" part inside the square brackets. But the columns part must "still be there", even if it stays empty.

Now, we don't want to copy and paste the values we learned from the `nrow()` calls, that's bad practice. We will just put the code inside the function:

```{r EWBurials binomial}

binom.test(nrow(EWBurials[EWBurials$Sex == "Male",]), 
           nrow(EWBurials), 
           0.5, alternative = "two.sided")
```

How can we interprete the result?
The p-value is 1! 

So there is a 100% chance that we are WRONG if we accept the alternative hypothesis... soo... yes, here we have a 50-50 distribution of men and women. No surprise for those who looked at the numbers before.

But that's how easy the binomial test is!



## chi squared test

Die erste Anwendung für den Chi-Quadrat-Test, die wir benutzen wollen, ist ähnlich wieder Binomialtest. ABER ich kann sie für deutlich mehr Werte anwenden als nur zwei. Aus Einfachheitsgründen machen wir das aber hier nochmal mit zwei Werten und einer einfachen Verteilung.

```{r chi goodness of fit test}
# die neolithischen Gräber
nl_G <- c(10,50)
# die Wahrscheinlichkeit, mit der sie sich eigentlich auf die zwei Gruppen verteilen sollten
Vert_fl <- c(0.404,0.596)
# voll simpel der Test
chisq.test(nl_G, p= Vert_fl)
```

Allerdings sind die Ergebnisse, die er uns gibt, nicht ganz so spannend, wie wenn wir es selber ausrechnen. Ich will zB unbedingt herausfinden, wie die Erwartungswerte sind. Dann kann ich nämlich auch erst abschätzen, ob ich den Test überhaupt nehmen sollte (Erinnerung: nicht mehr als 20% der Erwartunswerte sollten unter 5 fallen). 

Deswegen weise ich *den ganzen Test* einer Variablen zu. Und dann kann ich nachschauen, wie die Erwartungswerte aussehen und was mich noch alles interessiert.

```{r chi goodness of fit test 2}
# ich nenne die Variable chi
chi <- chisq.test(nl_G, p = Vert_fl)
chi$expected
chi$p.value
```
Ich kann v.a. auch coolerweise innerhalb meines R-markdown-Dokumentes den P-Wert aufrufen. Wenn ich \` r chi\$p.value \` schreibe, bekomme ich:  `r chi$p.value`.


Machen wir weiter mit etwas komplizierterem. Wie wär es wenn es mal mehr als nur zwei Ausprägungen der Variable geben dürfte? Ist doch langweilig, immer nur nach der Verteilung einer Variable zu schauen...

### Chi Quadrat mit zwei oder mehr Variablen!

Also wie wär es, wenn wir schauen, ob Grabbeigaben und Geschlecht zusammenhängen? In der Tabelle von EWBurials wurde nur das Vorhandensein oder die Abwesenheit von Grabbeigaben aufgenommen.

Der Chi-Quadrat-Test von R arbeitet am liebsten mit tables oder matrices (Matrizen, pl. von Matrix). 

Tables entstehen, wenn man die table-Funktion benutzt. Sie sind auch eine Arte Datenformat in R, das im Hintergrund anders aussieht als ein Dataframe. Muss uns nicht weiter beschäftigen, wir brauchen eh die Gegenüberstellung von Werten, wie wir sie mit table bekommen:

```{r chi}
# zuerst schauen wir einmal, wie diese Table aussieht:
table(EWBurials$Sex, EWBurials$Goods)
# und dann bauen wir sie einfach in den Befehl für den Chi-Quadrat-Test ein (chisq.test)
chisq.test(table(EWBurials$Sex, EWBurials$Goods))
```
Oje! Ein p-Wert von fast 0,5... was heißt das?

Eine Chance von 50% dass wir uns irren, wenn wir die Alternativhypothese annehmen wollen? 

Na toll.

Das ist ja richtig hilfreich.

Aber durchaus gar nicht so selten. Was können wir also sagen?

"Es gibt keine statistischen Hinweis darauf, dass die An- oder Abwesenheit von Grabbeigaben etwas mit dem Geschlecht zu tun hat."

Die Abweichungen sind ja auch wirklich gering.


Jetzt nehmen wir dochmal zwei Variablen, die mehr als nur eine Ausprägung haben. Zum Bsp die Altersgruppen "Age", wer in die Tabelle schaut, sieht, dass es da "Child", "Adolescent", "young Adult" und noch mehr Kategorien gibt. Haben diese evtl. etwas mit dem Auftreten der Grabbeigaben zu tun? Ist ja durchaus eine gängige archäologische These.

```{r Age and Grave goods}
tab_A_G <- table(EWBurials$Age, EWBurials$Goods)
chi_tab_A_G <- chisq.test(tab_A_G)
```

Wie muss dieser p-Wert interpretiert werden?

Schauen wir doch vorher nochmal kurz, ob wir den Test überhaupt machen können.
```{r Erwartungswerte}
chi_tab_A_G$expected
```

Najaaaa... eigentlich nicht. 

Warum nicht?

Was könnte man tun, um das Problem zu umgehen? Diskutiert das in eurer Gruppe!

#### Noch mal ein anderes Beispiel..

Nehmen wir dafür Daten von den Neanderthaler-Fundstellen Ferrassie und Castanet.

```{r}
data("EndScrapers") # Daten laden
```
 Da sind so schön viele nominale Variablen drin, die wir gegeneinander testen können. Schauen wir sie uns einmal an:
```
View(EndScrapers)
```

Hmmmh, aber da tauchen die Häufigkeitsangaben schon in einer extra Spalte auf. Wie kann ich damit umgehen?

Wer sich an das letzte Skript erinnert, erinnert sich jetzt vllt auch an "aggregate". Wir wollen vllt einfach mal untersuchen, ob Fundstelle und Retuschierungsgrad zusammenhängen. Dafür kommen jetzt die folgenden Schritte:

1. Ich summiere die Freq (also die Häufigkeiten) nach Site und Retouched. Das heißt, diese beiden Variablen werden behalten, alle anderen und wenn in ihnen der gleiche Wert vorkommt, wird Freq aufsummiert (alle anderen Variablen sind egal). Dadurch wird die Tabelle verkleinert

2. Ich schau mir die ersten Zeilen der Tabelle an, hat alles geklappt?

3. Ich muss jetzt die Tabelle so umformen, dass die Häufigkeiten unter den Fundstellennamen stehen. Dafür brauche ich `spread`.

4. Anschauen: Alles klar?

5. Leider ist jetzt immer noch alles ein dataframe. D.h. die Angaben unter "Retouched" sind immer noch ein Vektor (eine eigene Spalte) und keine Zeilenbenennung... was wir brauchen ist eine Matrix. Matrizen zeichnen sich dadurch aus, dass sie Spalten- und Zeilennamen haben. Dafür brauchen wir also folgende Schritte:

5.a) Ich mache aus den Angaben in Retouched die Namen für die einzelnen Zeilen

5. b) Ich lösche die Spalte Retouched

6. Jetzt habe ich eine Matrix. Damit kann chisq.test wunderbar umgehen. 

Ooookeee.. Los geht's:
```{r chi square data wrangling}
# 1.
end_site_ret <-  aggregate(EndScrapers$Freq, by = list(Site = EndScrapers$Site, Retouched = EndScrapers$Retouched), FUN = sum)
# dran denken, die Freq-Spalte wird jetzt x genannt
# 2.
head(end_site_ret) # head ist ein kleiner Trick, der mir nur die ersten paar Einträge eines data frames zeigt 
# 3. die Angaben unter x werden meine Werte in der Tabelle, die Site-Angaben werden meine Spaltenköpfe:
end_sr <- end_site_ret %>% 
  spread(value = "x", key = "Site")
# 4. alles klar?
head(end_sr)  # anschauen
# 5. a) rownames ist der Befehl: der data frame end_sr bekommt die rownames, die unter end_sr$Retouched liegen 
rownames(end_sr) <- end_sr$Retouched
head(end_sr) # einmal checken
# 5. b) jetzt brauch ich die Spalte end_sr$Retouched nicht mehr. weg damit:
end_sr <- end_sr[,-1]
head(end_sr)
```

Puuuh, aber jetzt wird es spannend: Klappt der Chi-Quadrat-Test?

```{r chi site retouched}
chi <- chisq.test(end_sr)
chi
chi$expected
```

Yaay!
Erfolg! 

Aber was ist das für ein komischer p-Wert ? "2.2e-16" ?

Das ist die wissenschaftliche Notation von 2,2 mal e hoch -16. 

- e ist die eulersche Zahl, rund 2,718 

- hoch minus etwas ist ein Bruch: 2⁻² = 2/2² = 2/4 = 1/2

- 2,2 * 2,718⁻¹⁶ = 2,2/2.781¹⁶ = winzig klein

p ist also winzig klein! Yeah! 

Wie interpretier ich das?

## Stärke eines Zusammenhangs

Um die **Stärke** eines Zusammenhangs zu messen, kann ich nicht den p-Wert des Chi-Quadrat-Tests nehmen. 

Dafür nehme ich am besten Cramér's V... oder Cramérs Index (CI) oder K...  manchmal bennenen Statistiker Sachen unterschiedlich und das ist voll nervig. Aber egal. Alles das gleiche.

Es gibt mehrere Pakete, die man installieren kann, die diese Berechnungen anstellen, in `base r` ist das leider nicht umgesetzt. Wir installieren "lsr", das Paket for *Learning Statitics with R*.

```{r vcd}
library(lsr)
```

Im Paket lsr gibt es die Funktion "cramersV". Lasst uns die mal ausprobieren. Wir geben ihr einfach die gleiche Matrix wie der Chi-Quadrat-Funktion:

```{r cramersV}
cramersV(end_sr)  
```
Und was sagt uns das Ergebnis nun?

Cramérs V kann Werte zwischen 0 und 1 annehmen. Ist V = 0, dann besteht kein Zusammenhang, ist V = 1, besteht ein vollständiger Zusammenhang. Der Wert von 0,39 ist gar nicht so übel. Es ist ein moderater Zusammenhang feststellbar.

## Herzlichen Glückwunsch!

Wir haben mit dem binomial-Test geschaut, ob sich eine Variable einer bestimmten Verteilung entsprechend verhält und haben dafür einen Wahrscheinlichkeitswert errechnet.

Wir haben den Chi-Quadrat-Test genutzt, um a) das gleiche wie mit dem binomial-Test zu überprüfen und b) die Abhängigkeit zweier Variablen zueinander zu untersuchen und auch dafür immer Warhscheinlichkeitswerte errechnet.

Und wir haben jetzt mit Cramérs V die Stärke eines Zusammenhangs berechnet.

Super! 




# Sind meine Daten normalverteilt?

## Q-Q-plot

Optisch die Normalverteilung abzuschätzen ist sinnvoll, dann hat man schon einmal eine Idee. 

Ein Q-Q-plot ist ein "Quantil-Quantil-Plot": Die Beobachtungswerte eines Merkmals werden der Größe nach geordnet und abgetragen. Als Vergleich dienen die Quantile der Normalverteilung (theoretische Verteilung), diese werden als Linie abgetragen. Wenn die abgetragene Messwertreihe normalverteilt ist, stimmen die empirischen und die theoretischen Quantile annähernd überein, d. h. die Werte liegen auf einer Diagonalen.

Große systematische Abweichungen von dieser Diagonalen geben einen Hinweis darauf, die Messwerte doch nicht normalverteilt sind. Das Quantil-Quantil-Diagramm kann keinen Verteilungstest ersetzen. 

Das Paket, in dem Q-Q-plots für R umgesetzt wurden, heißt `ggpubr`, installieren wir es also:

```{r,eval = FALSE}
install.packages("ggpubr")
```

Und wenden es an. Wir schauen uns die Randdurchmesser der bronzezeitlichen Tassen aus Italien an, die im Datensatz `BACups` liegen:

```{r Q-Q-Plot}
library(ggpubr)
library(archdata)
data("BACups")
ggqqplot(BACups$RD)
```
Man sieht: auf der x-Achse wird die theoretische Verteilung abgetragen und auf der y-Achse die unserer Stichprobe (sample). Die Linie zeigt den Idealverlauf an.

Wie man sieht, weichen einige Punkte deutlich von der Linie und dem 95%-konfidenzintervall (das ist der graue Bereich) ab. Wir können also _nicht_ davon ausgehen, dass die Daten normalverteilt sind.

Testen wir das jetzt mit dem richtigen Test: Shapiro, los geht's!

## Shapiro-Wilk-Test

```{r test shapiro BACups_Randdurchmesser}
  shapiro.test(BACups$RD)
```

Ok, ok, was ist hier passiert? 

Der Shapiro-Wilk Test testet, ob es einen signifikanten Unterschied zwischen der Normalverteilung und den z-transformierten Daten der eigenen Messwertreihe gibt. Da der Test die Daten selber transformiert, müssen wir ihm nicht die z-transformierten Werte geben. 

Nehmen wir das Ergebnis einmal auseinander.

R sagt:

```
	Shapiro-Wilk normality test

data:  BACups$RD
W = 0.89773, p-value = 0.0001088
```

 - In der ersten Zeile noch einmal, welchen Test wir angewandt haben
 - in der zweiten Zeile, auf welchen Datensatz
 - und die dritte Zeile wird spannend. Da steht
  - W = 0.89773
  - p = 0.0001088

_W_ ist die in der Vorlesung erwähnte Testgröße, die mit einer theoretischen Verteilung im Bezug auf die Wahrscheinlichkeit abgeglichen wird, dass dieser Wert entsteht.

_p_ ist die Wahrscheinlichkeit, dass wir uns irren, wenn wir die H_0 - Hypothese ablehnen.

Was war H_0 noch einmal? 

H_0 ist: Es gibt keinen signifikatanten Unterschied zwischen der Normalverteilung und meiner Messwertreihe.

Wir irren uns also mit einer Chance von etwa 0,01 %, wenn wir das ablehnen und behaupten würden, es gäbe einen Unterschied zwischen der Normalverteilung und der Messwertreihe.

Umgedreht: Wir irren uns mit einer Wahrscheinlichkeit von 0,01%, wenn wir H_1 annehmen. Und H_1 war die Hypothese, DASS es einen Unterschied zwischen der theoretischen Verteilung und meinen Daten gibt. 

Ok. 

Der Test sagt also im Klartext: Hörmal, die Daten sind nicht normalverteilt. Die Chance, dass du dich irrst, wenn du davon ausgehst, dass sie NICHT normalverteilt sind, ist super gering.

Jetzt wissen wir aber, dass in unserem Datensatz zwei unterschiedliche Pasen enthalten sind. Wieso könnte das ein Problem sein? -- Diskutiert das in eurer Gruppe!

Wir untersuchen jetzt einmal nur die protoappeninen Tassen. Das heißt, wir filtern oder subsetten den originalen Datensatz:
```{r}
BACups_pa <- subset(BACups, BACups$Phase == "Protoapennine")
```
Und nutzen diesen noch einmal in dem Shapiro-Wilk-Test:

```
shapiro.test(BACups_pa$RD)
```

**Aufgabe: Was kommt hierbei heraus? Wie kann man das Ergebnis interpretieren? Wieso ist plötzlich das Thema des Signifikanzniveaus von Bedeutung? -- Diskutiert das in eurer Gruppe!**

Wenn wir herausgefunden haben, ob unsere Daten normalverteilt sind oder nicht, können wir ein geeignetes Testverfahren auswählen. Wir beginnen jetzt mit den Testverfahren, die wir nehmen können, wenn die Daten NICHT normalverteilt sind, den sogenannten non-parametrischen oder "verteilungsfreien" Testen:

# Gruppenunterschiede testen

## non-parametrische Teste

### U-Test

Der U-Test heißt in R `wilcox,test` und wir brauchen für ihn die beiden Gruppen als zwei unterschiedliche Datensätze. Nehmen wir doch einfach noch einmal die BACups und unterteilt in die beiden Phasen. Sind sie im Randdurchmesser unterschiedlich?

BACups_pa haben wir ja schon erstellt. Das noch einmal parallel für die Subappeninen Tassen:

```{r}
BACups_sa <- subset(BACups, BACups$Phase == "Subapennine")
```

und jetzt der Test:

```{r}
wilcox.test(BACups_pa$RD, BACups_sa$RD)
```

Ok, wie interpretieren wir das?
Als erstes sagt uns R noch einmal, welchen Test er gemacht hat. "continuity correction" macht er, weil wir den U-Test mit metrischen Daten durchgeführt haben. 
Dann gibt er die berechneten W und p - Werte und sagt uns netter Weise noch einmal, was die Alternativhypothese ist: `alternative hypothesis: true location shift is not equal to 0`. Die H_1 ist also, die beiden Gruppen sind unterschiedlich. p gibt uns die Irrtumswahrscheinlichkeit dafür.

Das Ergebnis ist also: Ja die beiden Gruppen unterscheiden sich höchstsignifikant (p < 0,01) voneinander im Randdurchmesser! Yay! Ein schönes Ergebnis!

Doch in welche Richtung unterscheiden sie sich?

Wir haben die Möglichkeit, mit dem Argument `alternative =` anzugeben, ob man schon davon ausgeht, dass eine Gruppe größer ist als die andere. Das heißt, ich kann eine gerichtet Alternativhypothese angeben. Man sagt immer "der erste Datensatz, den ich angebe" (x) ist "less", also kleiner, oder "greater", also größer, als der zweite. Oder man nimmt das Argument weg und berechnet einen "two.sided" Test, der keine vorherige Annahme trifft. Das haben wir eben gemacht.

Probieren wir also einmal aus, was passiert, wenn wir davon ausgehen, dass die protoappenninen Tassen kleiner sind als die subappenninen Tassen:

```{r}
wilcox.test(BACups_pa$RD, BACups_sa$RD, alternative = "less")
```
Und jetzt noch einmal andersherum: 
```{r}
wilcox.test(BACups_pa$RD, BACups_sa$RD, alternative = "greater")
```

**Aufgabe: Was bedeuten diese Ergebnisse? Visualisiert die Randdurchmesser doch einmal als Boxplotdiagramme. Versteht ihr die drei unterschiedlichen p-Werte jetzt besser? -- Diskutiert das in der Gruppe!** 



### K-S-Test

Der K-S-Test benötigt mindestens ordinal skalierte Daten (d.h. metrische gehen auch), man kann ihn als Anpassungstest verwenden und als Zweichstichproben-Test nutzen.

Wir nehmen ihn, um zwei Gruppen miteinander zu vergleichen. Auch hier kann man mit der `alternative` arbeiten.

Anhand des U-Tests und des Boxplot-Diagramms haben wir eben herausgefunden, welche Gruppe die größeren Randdurchmesser hat. Bauen wir das also in die Test-Berechnung ein und schauen, ob wir das gleiche Ergebnis wie mit dem U-Test bekommen:

```{r}
ks.test(BACups_pa$RD, BACups_sa$RD, alternative = "greater")
```

Zuerst sagt uns R wieder, welchen Test wir berechnet haben und welche Daten wir benutzt haben. Dann den D-Wert, der hier als `D^+` markiert ist, weil wir gesagt haben, dass wir mit der gerichteten Hypothese arbeiten, dass der erste Datensatz größer ist als der zweite. Genau das sagt uns R auch noch einmal: `alternative hypothesis: the CDF of x lies above that of y` , d.h. die CDF = "cumulative distribution function", die kumulative Verteilungsfunktion von x ist größer als die von y. 

Der p-Wert liegt noch immer unter 0,05, damit können wir einen hochsignifikanten Zusammenhang annehmen. Man merkt aber, dass das Ergebnis etwas "konservativer", also etwas vorsichtiger ist als das Ergebnis des U-Tests. 




# Rekapitulieren:

Um zu rekapitulieren: Wenn ich einen Datensatz analysieren möchte, muss ich erst gucken, ob er normalverteilt ist. Dafür kann ich ein QQ-Plot nutzen und den Shapiro-Test berechnen. Wenn die Daten nicht normalverteilt sind, muss ich einen non-parametrischen Test benutzen, zB den U-Test oder den KS-Test. Bei

## Hausaufgabe

```{r density Fibel Dicke des Bogens}

ggplot(Fibulae) +
  geom_density(aes(x = BT))

```
Die density-Funktion zeigt mir ein ganz überzeugendes Bild. Was sagt shapiro diesmal?

```{r shapiro Fibelbogendicke}
shapiro.test(Fibulae$BT)
```
Aha! Ein p-Wert von 0.58. Wie kann der interpretiert werden?

Super, jetzt wissen wir, wie wir am Anfang einmal überprüfen, ob unsere Daten normalverteilt sind. Wichtig wichtig für viele Testverfahren! 

--- 

Nehmen wir doch einfach die Area-Angaben aus dem Snodgrass-Datensatz und schauen, ob die Gruppen "innerhalb der Mauer" und "außerhalb der Mauer" signifikant unterschiedlich sind:
```{r}
snod_in <- subset(Snodgrass, Inside == "Inside")
snod_out <- subset(Snodgrass, Inside == "Outside" )

wilcox.test(snod_in$Area, snod_out$Area)
```

Der p-Wert ist wieder super klein, das heißt, die beiden Gruppen sind signifikant unterschiedlich!
Schauen wir uns das einmal an:

```{r}
ggplot(data = Snodgrass)+
  geom_boxplot(aes(y = Area, x = Inside, col = Inside))
  
```
Ich bin überzeugt. Das können wir wirklich stichhaltig interpretieren!
