---
title: "Korrelation und Regression"
author:
  - Schmidt, Sophie C.
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::word_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
---


```{r setup, include=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  comment = "#>",
  fig.path = "../figures/",
  fig.width=6, 
  fig.height=6
)
```

Pakete für heute:
```{r Pakete}
library(dplyr)
library(tidyr)
library(ggplot2)

library(archdata)
data("BACups")
data("EndScrapers")

if (!require("lsr")) install.packages("lsr")
library(lsr)

if (!require("ggpmisc")) install.packages("ggpmisc")
library(ggpmisc)

```

## Korrelation
Um Zusammenhänge zu erfassen, gibt es für jedes Skalenniveau unterschiedliche Methoden:

- für nominale Daten: Cramérs V

- für ordinale Daten: Kendalls Tau und Spearmans Rho

- für metrische Daten: Pearson-Bravais r

### für nominale Daten kurz wiederholt:

Cramérs V kennen wir schon. Nachdem der Chi-Qudrat-Test einen Zusammenhang aufgezeigt hat, können wir Cramérs V nutzen, um die Stärke des Zusammenhangs zu untersuchen. Chi-Quadrat ist in base vorinstalliert, für Cramérs V brauchen wir das Paket "lsr".

Wir machen hier die gleichen Schritte, wie vor ein paar Wochen, aber mit etwas anderen Daten. Der EndScraper-Datensatz eignet sich. Schauen wir, ob die Art der Rundung (Curvature) der Kratzer etwas mit der Art der Seiten zu tun hat.

Diese Art der Datenumformung braucht man häufig, deswegen machen wir das hier noch einmal.

```{r cramersV}

# 1. Zusammenfassen der Freq (Häufigkeiten) nach Curvature und Sides.
end_curv_sides <-  aggregate(EndScrapers$Freq, by = list(Curv = EndScrapers$Curvature, Sides = EndScrapers$Sides), FUN = sum)
# dran denken, die Freq-Spalte wird jetzt x genannt

# 2. anschauen
head(end_curv_sides)

# 3. die Angaben unter x werden meine Werte in der Tabelle, die Sides-Angaben sollen zu Rownames werden und die Curvature-Angaben werden meine Spaltenköpfe:
end_cs <- end_curv_sides %>% spread(value = "x", key = "Curv")

# 4. alles klar?
head(end_cs)

# 5. a) rownames ist der Befehl: der data frame end_sr bekommt die rownames, die unter end_sr$Sides liegen 
rownames(end_cs) <- end_cs$Sides
# anschauen:
head(end_cs)

# 5. b)  die Spalte löschen, die wir nicht mehr brauchen
end_cs <- end_cs[,-1]
#anschauen
head(end_cs)

chisq.test(end_cs)

cramersV(end_cs)
```

Wie kann man das Ergebnis interpretieren?

### Kendalls Tau

Rechnen wir doch einfach das Bsp aus der Präsentation nach: Wir haben einen Datensatz mit 4 Siedlungen, deren Anzahl von Öfen und Größe. Wir brauchen für den Test zwei Vektoren, die in der richtigen Reihenfolge diese beiden Wertereihen darstellen:

```{r Werte }
ha <-  c(2, 1, 4, 3.5)
nOfen <-  c(2, 3, 5, 1)
```

Jetzt geben wir diese in die Korrelationstestfunktion ein. R wählt automatisch die benötigte Version (a ist standard und b, wenn es Bindungen gibt). Über "alternative" kann man definieren, ob man schon glaubt, dass die erste oder zweite Gruppe größere Ränge einnimmt als die andere. Wir testen "two.sided", d.h. wir wissen das nicht:

```{r Kendalls Tau}
 cor.test(ha, nOfen, method = "kendall", alternative = "two.sided")
```

Sie mal einer an! Das Ergebnis ist ein anderes, wenn man meinen Rechenfehler nicht macht.

Ein letztes Wort zu Kendalls Tau: Wenn man sehr viele Daten hat (sehr lange Vektoren), die man testen möchte, kann die Berechnung sehr lange dauern (da ja jedes Paar gegeneinander getestet werden muss).  Dann informiert euch über Spearmans Rho (https://www.crashkurs-statistik.de/spearman-korrelation-rangkorrelation/), der ist eigentlich wie der folgende (Pearson und Bravais r), aber an Rängen wie Kendalls Tau. Er gilt als "weniger genau", aber für große Datensätze besser geeignet.

 Der Code ist simpel:
```{r}
 cor.test(ha, nOfen, method = "spearman", alternative = "two.sided")
```



Jetzt aber geht es noch um metrische Daten:

# Pearson und Bravais' R

Für diesen Test können wir den gleichen Befehl nutzen, müssen nur die Methode ändern. Und den Datensatz, denn mit Vektoren der Länge 4 ist der Pearson-und Bravais' R nicht glücklich.

Nehmen wir doch also einfach Rand- und Nackendurchmesser der bronzezeitlichen Tassen:

```{r}

 cor.test(BACups$RD, BACups$ND, method = "pearson", alternative = "two.sided")

```
Das Ergebnis sagt uns folgendes:

Der p-Wert ist sehr sehr klein, er wird über eine Testgröße t berechnet, die den Wert 53,87 hat. Ein dafür wichtiger Freiheitsgrad (df) hat den Wert 58.

Das Konfidenzintervall liegt bei einem R-Wert zwischen 0,98 und 0,99.

Der in diesem Test errechnete Korrelationskoeffizient ist 0,99.

Wir haben also einen sehr sicheren und sehr starken Zusammenhang zwischen Rand- und Nackendurchmesser.

Applaus!

Jetzt erstellen wir noch eine Linare Regression daraus:

## Lineare Regression

Die lineare Regression legt eine "best-fit"-Linie zwischen die Punkte. Sie soll möglichst gut den Punktverlauf abbilden.

In R ist das ziemlich einfach, in dem ich einem Streudiagramm den geom_smooth-Befehl mit der Methode "lm" (linear model) hinzufüge. Der Befehl "se = FALSE" sagt aus, dass ich jetzt gern *kein* Konfidenzintervall (standard error) dazu visualisieren möchte.

```{r lin reg}
ggplot(data = BACups)+
  geom_point(aes(x = RD, y = ND))+
  geom_smooth( aes(x = RD, y = ND), method = "lm",
              se = FALSE)
```

Einfach oder?

Dieses Diagramm können wir jetzt noch ein bisschen verbessern. Wir könnten 

1. doch das Konfidenzintervall angeben

2. dazuschreiben, wie diese Linie mathematisch beschrieben werden kann und angeben, wie der R²-Wert der Linie aussieht.

Also, fangen wir an mit 1.:

```{r lin reg + se}
ggplot(data = BACups)+
  geom_point(aes(x = RD, y = ND))+
  geom_smooth( aes(x = RD, y = ND), method = "lm",
              se = TRUE)
```
Denkbar einfach. Das Konfidenzintervall ist sehr schmal, was für eine gute Anpassung der Linie an die Punkte spricht.

Schauen wir uns doch einmal an, wie diese Linie mathematisch beschrieben werden und zusammen mit dem R²-Wert dem plot hinzugefügt werden kann.

Wir brauchen dafür das Paket "ggpmisc".

Dann fügen wir dem  bisherigen Plot (am besten ihr kopiert das bisherige einfach mit strg+c und strg+v) den Befehl "stat_poly_eq" hinzu. "Stat_poly_eq" kann die Statistik der equation (Formel) einfügen. Dafür braucht es erst einmal noch die  Eingabewerte der Regression (x =  und y = ), dann die Information, welcher Text als "label" in den Graphen hinzugefügt werden soll: Paste (füge ein) die Formel (..eq.label..,) und den R²-Wert (..adj.rr.label..) und separiere die beiden mit vier Leerzeichen (symbolisiert durch die Tilde). Formula  sind eine bestimmte Art von Objekten in R. An dieser Stelle sagt man mit  "formula = y~x", dass y  die abhängige Variable sein soll.  "parse = TRUE" bedeutet "ja bitte schreib es hin" und "size" gibt die Schriftgröße an. "label.y.npc"  platziert die Schrift und zwar auf der y-Achse nach Prozent (also in diesem Fall bei 70% der Y-Achse).


```{r lin reg + formel}

ggplot(data = BACups)+
  geom_point(aes(x = RD, y = ND))+
  geom_smooth( aes(x = RD, y = ND), method = "lm",
              se = TRUE)+
  stat_poly_eq(aes(x = RD, y = ND, label =  paste(..eq.label.., 
                                  ..adj.rr.label..,
                                  sep = "~~~~")),
               formula = y~x, # y sei die abhängige Variable
               parse = TRUE,
               size = 3,
               label.y.npc = 0.7)

```

Cool oder?

Wenn man diese Grafik noch mit Titel und schöner Achsenbeschriftung versieht, hat man echt eine publikationswürdige Grafik. 



